<!doctype html>

<html lang="en">
    <head>
		<link rel="stylesheet" href="style.css">
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async
          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
        </script>
		<meta charset="utf-8">
		<meta name="author" content="Jackson Hellmers, Cameron Craig">
		<meta name="description" content="CS766 Final Project Webpage">
        <title>CS766 Final Project</title>
    </head>

    <body>
        <div class="title_container">
            <div>
            <img id="crest" src="https://wisconsintechnologycouncil.com/wp-content/uploads/2019/11/color-UWcrest-print-scaled.png" alt="UW Academic Crest">
            </div>
            <div>
                <a href="https://github.com/jth1011/CS766Project">Repository</a>
                <a href="https://docs.google.com/presentation/d/138_rweMtrbvu958mmiug_Ikkh0QbJ4Xa9LWIaSgReYs/edit?usp=sharing">Presentation</a>
            </div>
        </div>
        
        <h1>CS766 Final Project</h1>
        <h2>Object Tracking Across Stitched Video Panoramas</h2>
        <div class="content-table">
        <ol>
            <li><a href="#introduction">Introduction</a></li>
            <li><a href="#dataset">Dataset</a></li>
            <li><a href="#architecture">Model Overview</a>
                <ol>
					<li><a href="#data-prep">Data Preparation</a></li>
                    <li><a href="#image-stitch">Image Stitching</a></li>
                    <li><a href="#object-tracking">Object Tracking</a></li>
                </ol>
            </li>
            <li><a href="#results">Results</a></li>
                <ol>
                    <li><a href="#evaluation-metric">Evaluation Metrics</a></li>
                    <li><a href="#successes">Successes</a></li>
                    <li><a href="#failures">Failures</a></li>
                </ol>
            <li><a href="#conclusion">Conclusion</a>
                <ol>
                    <li><a href="#observations">Observations</a></li>
                    <li><a href="#challenges">Challenges</a></li>
                    <li><a href="#future-work">Future Work</a></li>
                </ol>
            </li>
            <li><a href="#references">References</a></li>
        </ol>
        </div>
        
        <! Introduction Section>
        <div id="introduction">
            <h2>1.0 Introduction</h2>
            <div class="img-center">
			<figure>
			<img src="imgs/intro_pic.png">
			</figure>
			</div>
            <p>
				In this project we intend to implement an algorithm that automatically registers and stitches together the frames from multiple video sources to create a wide-angle or 360-degree video. This algorithm will complete the task using only the video frames (no other camera orientation data needed). The core assumption is that the images will all have been taken from approximately the same vantage point at the same time using one camera that pans from one side to the other. Image features will be found using a method known as SIFT [1], an algorithm which has been covered extensively in class. Using this stitched video, we intend to apply two different tracking algorithms to discover how they perform at the overlapping regions. The tracking algorithm we intend to use implements Kernelized Correlation Filters and hence is labeled as the KCF Tracker [2]. This object tracking method was chosen as it is both scale invariant and has the ability to recover from object occlusion, all while being a model that is lightweight enough to be run in real time [3].
            </p>

            <div class="img-center">
                <figure>
                    <img style="width:350px !important;" src="https://cdn11.bigcommerce.com/s-zbabt7ht4y/images/stencil/1280x1280/products/53060/81789/axis-01500-001-pic1__95502.1579690899.jpg?c=2&imbypass=on" alt="Axis P3719-PLE 360 degree IP security camera.">
                    <figcaption>Axis P3719-PLE 360 degree IP security camera.</figcaption>
                </figure>
            </div>

			<p>
				Tracking objects across multiple cameras has several real world applications, including surveillance cameras and entertainment. In recent years the surveillance camera industry has begun producing panoramic array security cameras. These cameras use multiple image sensors and multiple lenses contained within a single housing to simultaneously capture high resolution video across a wide field of view (sometimes up to 360 degrees). These cameras can be an efficient and cost effective solution when there is a need to cover a large, open area for surveillance. Compared to using multiple traditional cameras, just one panoramic camera may be able to view the entire area of interest. In order for the video captured by the cameras to be useful to an end-user, the separate video signals coming from the separate sensors should be combined in a way that allows the user to “pan” and “tilt” a viewport in software over a large continuous video canvas. This continuous canvas of video is achieved by stitching each frame of video from each camera. A powerful feature that has emerged in surveillance software is the ability to track the position of objects like persons, vehicles, high value items, etc. When combining this tracking technology with panoramic cameras, it is very important to characterize and understand how a real-time object tracking algorithm will perform when subjected to the various image artifacts that arise when stitching video.
			</p>

            <div class="img-center">
                <figure>
                    <img style="width:350px !important;" src="https://sporttechie-prod.s3.amazonaws.com/2016/11/Picture1kee.png" alt="Keemotion/Synergy Sports dual camera array.">
                    <figcaption>Keemotion/Synergy Sports dual camera array.</figcaption>
                </figure>
            </div>

            <p>
                Another important application that combines video stitching with object tracking is live sports camera automation. Many high school and college gymnasiums host a large number of different sporting events over the course of a year. While a few high-profile events may have the budget to afford a dedicated live television production, the vast majority of games do not have these resources. However, many games played in high school and college gyms still have fans that would like to watch from home. Especially during disruptions like the Covid-19 pandemic, it is important to provide fans with a way to enjoy these events remotely. One solution to this problem that has emerged in recent years is an automated live streaming camera system. While a static camera with a wide lens may be able to capture the action during a game, viewers usually prefer to see a more zoomed-in perspective that follows the action (e.g. panning left and right during a basketball game as players run back and forth). This system employs two or three static cameras mounted inside an enclosure, each with their optical centers placed close together, and angled to capture a different part of the playing surface. [5]
            </p>

            <div class="img-center">
                <figure>
                    <img style="width:550px !important;" src="https://simplecore.intel.com/newsroom/wp-content/uploads/sites/11/2019/03/keemotion-2x1.jpg" alt="Real-time player tracking on a stitched video canvas.">
                    <figcaption>Real-time player tracking on a stitched video canvas.</figcaption>
                </figure>
            </div>

            <p>
                The video signals from each camera are stitched onto a single image plane, and an object detection and tracking algorithm finds the locations of players in the game. The software fits a bounding box around the players, and, using a smoothing algorithm to avoid abrupt changes, pans a cropped viewing window back and forth on the image plane to simulate the motion of a camera on a tripod following the players. This cropped view can then be live-streamed to social media platforms to allow parents and other viewers to watch the game from home. Two key advantages of this system are that no human operator is required to operate the camera, and there are no moving parts in the system, which reduces mechanical wear over the life of the system.
            </p>
        </div>
        
        <! Dataset Section>
        <div id="dataset">
            <h2>2.0 Dataset</h2>
			<p>
				Our group was unable to find a proper dataset that contained split video files and ground truth bounding boxes so we made the decision to generate our own data. To accomplish this we first found a standard object tracking dataset that provided us with videos of single objects and the ground truth bounding boxes those objects were contained in. We ultimately decided on using LaSOT: Large-scale Single Object Tracking (found <a href="http://vision.cs.stonybrook.edu/~lasot/index.html">here</a>) which provides over 1500 videos belonging to 70 unique object categories. The average video length is approximately 2000 frames and each video offers a varying level of tracking difficulty by means of full or partial occlusion.
			</p>
			<div class="img-center">
			<figure>
			<img src="imgs/lasot_planes.png" alt="Example Frames From LaSOT Dataset">
			<figcaption>Example Frames from LaSOT Aircrafts Category</figcaption>
			</figure>
			</div>
        </div>
        
        <! Model Section>
        <div id="architecture">
            <h2>3.0 Model Overview</h2>
			<p>Model overview has been split up into three sections: Data Preparation, Image Stitching, and Object Tracking. In each section we will discuss the mathematical and logical reasons behind the decisions we made in the construction of our model. Additionally, we will briefly touch on how certain elements of the model were implemented in the code, but to better understand the inner workings of the entire project please visit our repository <a href="https://github.com/jth1011/CS766Project">here</a>.</p>
        </div>
		
		<! Data Prep Subsection>
        <div class="subsection" id="data-prep">
            <h3>3.1 Data Preparation</h3>
			<p>
				In order to have videos that we could stitch back together, we first had to take each video and split it into a left and right video source. To allow for proper reconstruction, each frame of the split videos was given an overlap. As the size of the overlap increased, the number of optimal matched points between the two videos increased as well. Each frame in the split videos had a random rotation and translation applied to it in an attempt to add some noise. We quickly realized that static augmentations were not putting a great deal of pressure on our model, as the transformation matrix would only need to be calculated once for the first pair of frames. To counter this, additional rotations and translations were applied to consecutive frames. An example input and output pair are shown below.
			</p>
			<div class="container3">
				<figure>
				  <img src="imgs/data_orig.gif">
				  <figcaption>Original Video</figcaption>
				</figure>
				<figure>
				  <img src="imgs/data_left.gif">
				  <figcaption>Random Augmentation: Left</figcaption>
				</figure>
				<figure>
				  <img src="imgs/data_right.gif">
				  <figcaption>Random Augmentation: Right</figcaption>
				</figure>
			</div>
			<p>
				Furthermore, we wanted to simulate the differences between camera sensors and settings by applying some color and brightness distortion to each frame for one of the videos. This created a distinct boundary between the left and right video sources that needed to be smoothened out in the section of our model dealing with image stitching.
			</p>
        </div>
        
        <! Panorma Subsection>
        <div class="subsection" id="image-stitch">
            <h3>3.2 Image Stitching</h3>
			<p>
				To stitch the split frames back together we need any algorithm that is able to extract a set of matching points found within both frames. There exist many algorithms that can accomplish such a task, but we decided on using the SIFT algorithm as it offers a quick and efficient framework that we are both familiar with implementing. However, not all point pairs returned by the SIFT algorithm are going to be optimal. At its base, image stitching is an overdetermined least squares problem which attempts to solve the following equation, where h represents our homography matrix. $$ A\vec{h} = \vec{0} $$ To further constrain the problem we restrict h such that it can only have unit norm. With this new constrained least squares problem we are able to redefine our equation in the following form with the eigenvector corresponding to the smallest eigenvalue being the vector which minimizes the least squares loss. $$ A^{T}A \vec{h} = \lambda \vec{h} $$ However, minimizing square loss does not gurantee that we have found the most optimal solution as the SIFT algorithm is likely to return some noneoptimal point pairs. Alternatively, iterating through all matched SIFT points and finding the global minimum is a computationally expensive process, so instead we implement random sampling (RANSAC). 
			</p>
            <p>
                Using the SIFT and RANSAC implementations detailed above we were able to reliably generate real-time stitched frames using the data from the split video sources. A side effect of the random rotations and homography projection was that each stitched video had large black boxes on the perimeter. While this would not be an issue for typical usage, we found the offset shifted the image enough that the ground truth bounding boxes we needed to evaluate our model were no longer were in the correct locations.  To move the bounding boxes back to their correct location we apply a dynamic crop that works to remove any blank areas from the  final stitched video. Shown below are examples of output videos before and after the cropping occurs.
            </p>
			<div class="container2">
				<figure>
				  <img src="imgs/data_uncropped.gif">
				  <figcaption>Uncropped</figcaption>
				</figure>
				<figure>
				  <img src="imgs/data_cropped.gif">
				  <figcaption>Cropped</figcaption>
				</figure>
			</div>
            <p>
                As mentioned earlier in the Data Preparation section, in an attempt to simulate inputs from different recording devices we added random color distortion and brightness offsets to the frames in the right split video. Our final step in the image stitching process was to smooth out the overlap between the left and right frames to make the distinct boundary less apparent. This is achieved by taking an average of the pixels within the overlap. Pixels closer to certain edge of the overlap will be influenced greater by information from that image (e.g. left pixels and left image). The following videos outline the contrast between smoothened and unsmoothened frames.
            </p>
			<div class="container2">
				<figure>
				  <img src="imgs/data_no_blend.gif">
				  <figcaption>Unblended</figcaption>
				</figure>
				<figure>
				  <img src="imgs/data_blend.gif">
				  <figcaption>Blended</figcaption>
				</figure>
			</div>
        </div>
        
        <! Object Tracking Subsection>
        <div class="subsection" id="object-tracking">
            <h3>3.3 Object Tracking</h3>
			<p>
				Our tracking algorithm makes use of kernelized filters to follow objects from frame to frame. By initializing the tracker with the ground truth bounding box from the first frame of the original video, the algorithm is able to learn the general structure and color of the tracked object. Using this information, the algorithm is able to make predictions on the objects location by finding areas of the video that correlate most greatly with the learned filter. With each consecutive frame the algorithm is better able to update its learned filter by using a combination data collected from all previous and current frames [2]. Illustrated below is the general architecture of the KCF model we implemented.
			</p>
			<div class="img-center">
			<figure>
			<img src="imgs/kcf_tracker.png">
			<figcaption>KCF Tracker Architecture</figcaption>
			</figure>
			</div>
            <p>
                Examining the figure we are able to extract a few of the highlights and drawbacks of the model. The first major takeaway is the models ability to store and remember previous kernel filters. Becuase of this the tracking algorithm is resistant to object occlusion and dynamic object velocities. The model does not rely on continuity as each update looks for the tracked object on a global scale instead of locally. Additionally, this allow the model to be invariant to changes in scale when using a scale-invariant kernel function to generate the filters [3]. One of the main drawbacks of this model is its inability to track objects that have a fluid or variable shape/structure. Knowing that the kernelized filters are constructed using information only present in previous frames, it makes sense that an object undergoing a rapid transition or transformation might cause the algorithm to misidentify the tracked object. A few of the failure cases highlight this issue by failing to track a plane after it experiences a swift change in brightness and color [3].
            </p>
        </div>
        
        <! Results Section>
        <div id="results">
            <h2>4.0 Results</h2>
			<p>
				
			</p>
        </div>
        <div class="subsection" id="evaluation-metric">
            <h3>4.1 Evaluation Metrics</h3>
            <p>
                We do not intend to empirically evaluate the stitched video frames and instead will primarily be testing the
                effectiveness of the tracking algorithms. In order to develop the metrics needed for evaluation we will
                generate footage with ground truth bounding boxes around the tracked object. The output of the tracking
                algorithms can then be compared to the ground truth by assessing the commonly used Intersection over
                Union (IOU) distance metric. By taking the ratio of the intersection of the bounding boxes against their union
                we get a simple metric on the range of [0, 1], with an optimally predicted box returning a value of 1.
            </p>
            <p>
                NEED TO ADD IMAGE OF IOU EXAMPLE
            </p>
        </div>
        <div class="subsection" id="successes">
            <h3>4.2 Successes</h3>
			<div class="container2">
				<figure>
				  <img src="imgs/track_orig.gif">
				  <figcaption>Original Tracking</figcaption>
				</figure>
				<figure>
				  <img src="imgs/track_stitched.gif">
				  <figcaption>Stitched Tracking</figcaption>
				</figure>
			</div>
        </div>
        <div class="subsection" id="failures">
            <h3>4.3 Failures</h3>
			<div class="container2">
				<figure>
				  <img src="imgs/fail_bright.gif">
				  <figcaption>Failure 1</figcaption>
				</figure>
				<figure>
				  <img src="imgs/fail_overlap.gif">
				  <figcaption>Failure 2</figcaption>
				</figure>
			</div>
        </div>
        
        <! Conclusion Section>
        <div id="conclusion">
            <h2>5.0 Conclusion</h2>
        </div>
        
        <! Observations Subsection>
        <div class="subsection" id="observations">
            <h3>5.1 Observations</h3>
        </div>
        
        <! Challenges Subsection>
        <div class="subsection" id="challenges">
            <h3>5.2 Challenges</h3>
        </div>
        
        <! Future Work Subsection>
        <div class="subsection" id="future-work">
            <h3>5.3 Future Work</h3>
        </div>
        
        <! References Section>
        <div id="references">
            <h2>6.0 References</h2>
			<p>
			<ol>
				<li><p class="ref-indent">Yanfang Li, Yaming Wang, Wenqing Huang and Zuoli Zhang, <b>"Automatic image stitching using SIFT."</b> 2008 International Conference on Audio, Language and Image Processing, pp. 568-571. IEEE, 2008.</p></li>
				<li><p class="ref-indent">Bolme, David S., J. Ross Beveridge, Bruce A. Draper, and Yui Man Lui. <b>"Visual object tracking using adaptive correlation filters."</b> 2010 IEEE computer society conference on computer vision and pattern recognition, pp. 2544-2550. IEEE, 2010.</p></li>
				<li><p class="ref-indent">Yang, Yuebin, and Guillaume-Alexandre Bilodeau. <b>"Multiple object tracking with kernelized correlation filters in urban mixed traffic."</b> 2017 14th Conference on Computer and Robot Vision (CRV), pp. 209-216. IEEE, 2017.</p></li>
				<li><p class="ref-indent">Held, David & Thrun, Sebastian & Savarese, Silvio. (2016). <b>"Learning to Track at 100 FPS with Deep Regression Networks."</b> https://arxiv.org/abs/1604.01802</p></li> 
                <li><p class="ref-indent"><b>“Live sports production,”</b> Synergy Sports, 14-Jan-2022. [Online]. Available: https://synergysports.com/solutions/live-sport-production/.</p></li> 
			</ol>
			</p>
        </div>
    </body>
</html>
