<!doctype html>

<html lang="en">
    <head>
		<link rel="stylesheet" href="style.css">
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async
          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
        </script>
		<meta charset="utf-8">
		<meta name="author" content="Jackson Hellmers, Cameron Craig">
		<meta name="description" content="CS766 Final Project Webpage">
        <title>CS766 Final Project</title>
    </head>

    <body>
        <div class="title_container">
            <div>
            <img id="crest" src="https://wisconsintechnologycouncil.com/wp-content/uploads/2019/11/color-UWcrest-print-scaled.png" alt="UW Academic Crest">
            </div>
            <div>
                <a href="https://github.com/jth1011/CS766Project">Repository</a>
                <a href="https://docs.google.com/presentation/d/138_rweMtrbvu958mmiug_Ikkh0QbJ4Xa9LWIaSgReYs/edit?usp=sharing">Presentation</a>
            </div>
        </div>
        
        <h1>CS766 Final Project</h1>
        <h2>Object Tracking Across Stitched Video Panoramas</h2>
        <div class="content-table">
        <ol>
            <li><a href="#introduction">Introduction</a></li>
            <li><a href="#dataset">Dataset</a></li>
            <li><a href="#architecture">Model Overview</a>
                <ol>
					<li><a href="#data-prep">Data Preparation</a></li>
                    <li><a href="#image-stitch">Image Stitching</a></li>
                    <li><a href="#object-tracking">Object Tracking</a></li>
                </ol>
            </li>
            <li><a href="#results">Results</a></li>
                <ol>
                    <li><a href="#evaluation-metric">Evaluation Metrics</a></li>
                    <li><a href="#successes">Successes</a></li>
                    <li><a href="#failures">Failures</a></li>
                </ol>
            <li><a href="#conclusion">Conclusion</a>
                <ol>
                    <li><a href="#observations">Observations</a></li>
                    <li><a href="#challenges">Challenges</a></li>
                    <li><a href="#future-work">Future Work</a></li>
                </ol>
            </li>
            <li><a href="#references">References</a></li>
        </ol>
        </div>
        
        <! Introduction Section>
        <div id="introduction">
            <h2>1.0 Introduction</h2>
            <div class="img-center">
			<figure>
			<img src="imgs/intro_pic.png">
			</figure>
			</div>
            <p>
				In this project we intend to implement an algorithm that automatically registers and stitches together the frames from multiple video sources to create a wide-angle or 360-degree video. This algorithm will complete the task using only the video frames (no other camera orientation data needed). The core assumption is that the images will all have been taken from approximately the same vantage point at the same time using one camera that pans from one side to the other. Image features will be found using a method known as SIFT [1], an algorithm which has been covered extensively in class. Using this stitched video, we intend to apply two different tracking algorithms to discover how they perform at the overlapping regions. The tracking algorithm we intend to use implements Kernelized Correlation Filters and hence is labeled as the KCF Tracker [2]. This object tracking method was chosen as it is both scale invariant and has the ability to recover from object occlusion, all while being a model that is lightweight enough to be run in real time [3].
            </p>
			<p>
				Tracking objects across multiple cameras has several real world applications in the fields of security and entertainment.
			</p>
        </div>
        
        <! Dataset Section>
        <div id="dataset">
            <h2>2.0 Dataset</h2>
			<p>
				Our group was unable to find a proper dataset that contained split video files and ground truth bounding boxes so we made the decision to generate our own data. To accomplish this we first found a standard object tracking dataset that provided us with videos of single objects and the ground truth bounding boxes those objects were contained in. We ultimately decided on using LaSOT: Large-scale Single Object Tracking (found <a href="http://vision.cs.stonybrook.edu/~lasot/index.html">here</a>) which provides over 1500 videos belonging to 70 unique object categories. The average video length is approximately 2000 frames and each video offers a varying level of tracking difficulty by means of full or partial occlusion.
			</p>
			<div class="img-center">
			<figure>
			<img src="imgs/lasot_planes.png" alt="Example Frames From LaSOT Dataset">
			<figcaption>Example Frames from LaSOT Aircrafts Category</figcaption>
			</figure>
			</div>
        </div>
        
        <! Model Section>
        <div id="architecture">
            <h2>3.0 Model Overview</h2>
			<p>Model overview has been split up into three sections: Data Preparation, Image Stitching, and Object Tracking. In each section we will discuss the mathematical and logical reasons behind the decisions we made in the construction of our model. Additionally, we will briefly touch on how certain elements of the model were implemented in the code, but to better understand the inner workings of the entire project please visit our repository <a href="https://github.com/jth1011/CS766Project">here</a>.</p>
        </div>
		
		<! Data Prep Subsection>
        <div class="subsection" id="data-prep">
            <h3>3.1 Data Preparation</h3>
			<p>
				In order to have videos that we could stitch back together, we first had to take each video and split it into a left and right video source. To allow for proper reconstruction, each frame of the split videos was given an overlap. As the size of the overlap increased, the number of optimal matched points between the two videos increased as well. Each frame in the split videos had a random rotation and translation applied to it in an attempt to add some noise. We quickly realized that static augmentations were not putting a great deal of pressure on our model, as the transformation matrix would only need to be calculated once for the first pair of frames. To counter this, additional rotations and translations were applied to consecutive frames. An example input and output pair are shown below.
			</p>
			<div class="container3">
				<figure>
				  <img src="imgs/data_orig.gif">
				  <figcaption>Original Video</figcaption>
				</figure>
				<figure>
				  <img src="imgs/data_left.gif">
				  <figcaption>Random Augmentation: Left</figcaption>
				</figure>
				<figure>
				  <img src="imgs/data_right.gif">
				  <figcaption>Random Augmentation: Right</figcaption>
				</figure>
			</div>
			<p>
				Furthermore, we wanted to simulate the differences between camera sensors and settings by applying some color and brightness distortion to each frame for one of the videos. This created a distinct boundary between the left and right video sources that needed to be smoothened out in the section of our model dealing with image stitching.
			</p>
        </div>
        
        <! Panorma Subsection>
        <div class="subsection" id="image-stitch">
            <h3>3.2 Image Stitching</h3>
			<p>
				To stitch the split frames back together we need any algorithm that is able to extract a set of matching points found within both frames. There exist many algorithms that can accomplish such a task, but we decided on using the SIFT algorithm as it offers a quick and efficient framework that we are both familiar with implementing. However, not all point pairs returned by the SIFT algorithm are going to be optimal. At its base, image stitching is an overdetermined least squares problem which attempts to solve the following equation, where h represents our homography matrix. $$ A\vec{h} = \vec{0} $$ To further constrain the problem we restrict h such that it can only have unit norm. With this new constrained least squares problem we are able to redefine our equation in the following form with the eigenvector corresponding to the smallest eigenvalue being the vector which minimizes the least squares loss. $$ A^{T}A \vec{h} = \lambda \vec{h} $$ However, minimizing square loss does not gurantee that we have found the most optimal solution as the SIFT algorithm is likely to return some noneoptimal point pairs. Alternatively, iterating through all matched SIFT points and finding the global minimum is a computationally expensive process, so instead we implement random sampling (RANSAC). 
			</p>
            <p>
                Using the SIFT and RANSAC implementations detailed above we were able to reliably generate real-time stitched frames using the data from the split video sources. A side effect of the random rotations and homography projection was that each stitched video had large black boxes on the perimeter. While this would not be an issue for typical usage, we found the offset shifted the image enough that the ground truth bounding boxes we needed to evaluate our model were no longer were in the correct locations.  To move the bounding boxes back to their correct location we apply a dynamic crop that works to remove any blank areas from the  final stitched video. Shown below are examples of output videos before and after the cropping occurs.
            </p>
			<div class="container2">
				<figure>
				  <img src="imgs/data_uncropped.gif">
				  <figcaption>Uncropped</figcaption>
				</figure>
				<figure>
				  <img src="imgs/data_cropped.gif">
				  <figcaption>Cropped</figcaption>
				</figure>
			</div>
            <p>
                As mentioned earlier in the Data Preparation section, in an attempt to simulate inputs from different recording devices we added random color distortion and brightness offsets to the frames in the right split video. Our final step in the image stitching process was to smooth out the overlap between the left and right frames to make the distinct boundary less apparent. This is achieved by taking an average of the pixels within the overlap. Pixels closer to certain edge of the overlap will be influenced greater by information from that image (e.g. left pixels and left image). The following videos outline the contrast between smoothened and unsmoothened frames.
            </p>
			<div class="container2">
				<figure>
				  <img src="imgs/data_no_blend.gif">
				  <figcaption>Unblended</figcaption>
				</figure>
				<figure>
				  <img src="imgs/data_blend.gif">
				  <figcaption>Blended</figcaption>
				</figure>
			</div>
        </div>
        
        <! Object Tracking Subsection>
        <div class="subsection" id="object-tracking">
            <h3>3.3 Object Tracking</h3>
			<p>
				Our tracking algorithm makes use of kernelized filters to follow objects from frame to frame. By initializing the tracker with the ground truth bounding box from the first frame of the original video, the algorithm is able to learn the general structure and color of the tracked object. Using this information, the algorithm is able to make predictions on the objects location by finding areas of the video that correlate most greatly with the learned filter. With each consecutive frame the algorithm is better able to update its learned filter by using a combination data collected from all previous and current frames [2]. Illustrated below is the general architecture of the KCF model we implemented.
			</p>
			<div class="img-center">
			<figure>
			<img src="imgs/kcf_tracker.png">
			<figcaption>KCF Tracker Architecture</figcaption>
			</figure>
			</div>
            <p>
                Examining the figure we are able to extract a few of the highlights and drawbacks of the model. The first major takeaway is the models ability to store and remember previous kernel filters. Becuase of this the tracking algorithm is resistant to object occlusion and dynamic object velocities. The model does not rely on continuity as each update looks for the tracked object on a global scale instead of locally. Additionally, this allow the model to be invariant to changes in scale when using a scale-invariant kernel function to generate the filters [3]. One of the main drawbacks of this model is its inability to track objects that have a fluid or variable shape/structure. Knowing that the kernelized filters are constructed using information only present in previous frames, it makes sense that an object undergoing a rapid transition or transformation might cause the algorithm to misidentify the tracked object. A few of the failure cases highlight this issue by failing to track a plane after it experiences a swift change in brightness and color [3].
            </p>
        </div>
        
        <! Results Section>
        <div id="results">
            <h2>4.0 Results</h2>
			<p>
				
			</p>
        </div>
        <div class="subsection" id="evaluation-metric">
            <h3>4.1 Evaluation Metrics</h3>
            <p>
                We do not intend to empirically evaluate the stitched video frames and instead focused primarily on testing the
                effectiveness of the tracking algorithms. We compared the output of the tracking algorithms against the ground truth by assessing the commonly used Intersection over Union (IoU) metric. This metric takes the ratio of the intersection of the bounding boxes against their union yielding a range of [0, 1], with an optimally predicted box returning a value of 1. Shown below is the equation for calculating the IoU as well as a few examples demonstrating varying degrees of bounding box overlap.
            </p>
            <div class="img-center">
			<figure>
			<img src="imgs/iou1.png">
			<figcaption>Mathematical Breakdown of IoU</figcaption>
			</figure>
			</div>
			<div class="img-center">
			<figure>
			<img src="imgs/iou2.png">
			<figcaption>Example of a Bad (Left), Mediocre (Middle), and Excellent (Right) IoU</figcaption>
			</figure>
			</div>
        </div>
        <div class="subsection" id="successes">
            <h3>4.2 Successes</h3>
			<p>
				We found that a majoirty of our tests performed very well. 
			</p>
			<div class="container2">
				<figure>
				  <img src="imgs/track_orig.gif">
				  <figcaption>Original Tracking</figcaption>
				</figure>
				<figure>
				  <img src="imgs/track_stitched.gif">
				  <figcaption>Stitched Tracking</figcaption>
				</figure>
			</div>
        </div>
        <div class="subsection" id="failures">
            <h3>4.3 Failures</h3>
			<p>
				
			</p>
			<div class="container2">
				<figure>
				  <img src="imgs/fail_bright.gif">
				  <figcaption>Failure 1</figcaption>
				</figure>
				<figure>
				  <img src="imgs/fail_overlap.gif">
				  <figcaption>Failure 2</figcaption>
				</figure>
			</div>
        </div>
        
        <! Conclusion Section>
        <div id="conclusion">
            <h2>5.0 Conclusion</h2>
        </div>
        
        <! Observations Subsection>
        <div class="subsection" id="observations">
            <h3>5.1 Observations</h3>
        </div>
        
        <! Challenges Subsection>
        <div class="subsection" id="challenges">
            <h3>5.2 Challenges</h3>
        </div>
        
        <! Future Work Subsection>
        <div class="subsection" id="future-work">
            <h3>5.3 Future Work</h3>
        </div>
        
        <! References Section>
        <div id="references">
            <h2>6.0 References</h2>
			<p>
			<ol>
				<li><p class="ref-indent">Yanfang Li, Yaming Wang, Wenqing Huang and Zuoli Zhang, <b>"Automatic image stitching using SIFT."</b> 2008 International Conference on Audio, Language and Image Processing, pp. 568-571. IEEE, 2008.</p></li>
				<li><p class="ref-indent">Bolme, David S., J. Ross Beveridge, Bruce A. Draper, and Yui Man Lui. <b>"Visual object tracking using adaptive correlation filters."</b> 2010 IEEE computer society conference on computer vision and pattern recognition, pp. 2544-2550. IEEE, 2010.</p></li>
				<li><p class="ref-indent">Yang, Yuebin, and Guillaume-Alexandre Bilodeau. <b>"Multiple object tracking with kernelized correlation filters in urban mixed traffic."</b> 2017 14th Conference on Computer and Robot Vision (CRV), pp. 209-216. IEEE, 2017.</p></li>
				<li><p class="ref-indent">Held, David & Thrun, Sebastian & Savarese, Silvio. (2016). <b>"Learning to Track at 100 FPS with Deep Regression Networks."</b> https://arxiv.org/abs/1604.01802</p></li> 
			</ol>
			</p>
        </div>
    </body>
</html>
