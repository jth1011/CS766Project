<!doctype html>

<html lang="en">
    <head>
		<link rel="stylesheet" href="style.css">
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async
          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
        </script>
		<meta charset="utf-8">
		<meta name="author" content="Jackson Hellmers, Cameron Craig">
		<meta name="description" content="CS766 Final Project Webpage">
        <title>CS766 Final Project</title>
    </head>

    <body>
        <div class="title_container">
            <div>
            <img id="crest" src="https://wisconsintechnologycouncil.com/wp-content/uploads/2019/11/color-UWcrest-print-scaled.png" alt="UW Academic Crest">
            </div>
            <div>
                <a href="https://github.com/jth1011/CS766Project">Repository Link</a>
                <a href="https://docs.google.com/presentation/d/138_rweMtrbvu958mmiug_Ikkh0QbJ4Xa9LWIaSgReYs/edit?usp=sharing">Presentation Link</a>
            </div>
        </div>
        
        <h1>CS766 Final Project</h1>
        <h2>Object Tracking Across Stitched Video Panoramas</h2>
        <div class="content-table">
        <ol>
            <li><a href="#introduction">Introduction</a></li>
            <li><a href="#dataset">Dataset</a></li>
            <li><a href="#architecture">Model Overview</a>
                <ol>
					<li><a href="#data-prep">Data Preparation</a></li>
                    <li><a href="#image-stitch">Image Stitching</a></li>
                    <li><a href="#object-tracking">Object Tracking</a></li>
                </ol>
            </li>
            <li><a href="#results">Results</a></li>
            <li><a href="#conclusion">Conclusion</a>
                <ol>
                    <li><a href="#observations">Observations</a></li>
                    <li><a href="#challenges">Challenges</a></li>
                    <li><a href="#future-work">Future Work</a></li>
                </ol>
            </li>
            <li><a href="#references">References</a></li>
        </ol>
        </div>
        
        <! Introduction Section>
        <div id="introduction">
            <h2>1.0 Introduction</h2>
            <div class="img-center">
			<figure>
			<img src="imgs/intro_pic.png">
			</figure>
			</div>
            <p>
				In this project we intend to implement an algorithm that automatically registers and stitches together the frames from multiple video sources to create a wide-angle or 360-degree video. This algorithm will complete the task using only the video frames (no other camera orientation data needed). The core assumption is that the images will all have been taken from approximately the same vantage point at the same time using one camera that pans from one side to the other. Image features will be found using a method known as SIFT [1], an algorithm which has been covered extensively in class. Using this stitched video, we intend to apply two different tracking algorithms to discover how they perform at the overlapping regions. The tracking algorithm we intend to use implements Kernelized Correlation Filters and hence is labeled as the KCF Tracker [2]. This object tracking method was chosen as it is both scale invariant and has the ability to recover from object occlusion, all while being a model that is lightweight enough to be run in real time [3].
            </p>
			<p>
				Tracking objects across multiple cameras has several real world applications in the fields of security and entertainment.
			</p>
        </div>
        
        <! Dataset Section>
        <div id="dataset">
            <h2>2.0 Dataset</h2>
			<p>
				Our group was unable to find a proper dataset that contained split video files and ground truth bounding boxes so we made the decision to generate our own data. To accomplish this we first found a standard object tracking dataset that provided us with videos of single objects and the ground truth bounding boxes those objects were contained in. We ultimately decided on using LaSOT: Large-scale Single Object Tracking (found <a href="http://vision.cs.stonybrook.edu/~lasot/index.html">here</a>) which provides over 1500 videos belonging to 70 unique object categories. The average video length is approximately 2000 frames and each video offers a varying level of tracking difficulty by means of full or partial occlusion.
			</p>
			<div class="img-center">
			<figure>
			<img src="imgs/lasot_planes.png" alt="Example Frames From LaSOT Dataset">
			<figcaption>Example Frames from LaSOT Aircrafts Category</figcaption>
			</figure>
			</div>
        </div>
        
        <! Model Section>
        <div id="architecture">
            <h2>3.0 Model Overview</h2>
			<p>Model overview has been split up into three sections: Data Preparation, Image Stitching, and Object Tracking. In each section we will discuss the mathematical and logical reasons behind the decisions we made in the construction of our model. Additionally, we will briefly touch on how certain elements of the model were implemented in the code, but to better understand the inner workings of the entire project please visit our repository <a href="https://github.com/jth1011/CS766Project">here</a>.</p>
        </div>
		
		<! Data Prep Subsection>
        <div class="subsection" id="data-prep">
            <h3>3.1 Data Preparation</h3>
			<p>
				In order to have videos that we could stitch back together, we first had to take each video and split it into a left and right video source. To allow for proper reconstruction, each frame of the split videos was given an overlap. As the size of the overlap increased, the number of optimal matched points between the two videos increased as well. Each frame in the split videos had a random rotation and translation applied to it in an attempt to add some noise. We quickly realized that static augmentations were not putting a great deal of pressure on our model, as the transformation matrix would only need to be calculated once for the first pair of frames. To counter this, additional rotations and translations are applied to consecutive frames. An example input and output pair are shown below.
			</p>
			<div class="container3">
				<figure>
				  <img src="imgs/data_orig.gif">
				  <figcaption>Original Video</figcaption>
				</figure>
				<figure>
				  <img src="imgs/data_left.gif">
				  <figcaption>Random Augmentation: Left</figcaption>
				</figure>
				<figure>
				  <img src="imgs/data_right.gif">
				  <figcaption>Random Augmentation: Right</figcaption>
				</figure>
			</div>
			<p>
				Furthermore, we wanted to simulate the differences between camera sensors and settings by applying some color and brightness distortion to each frame for one of the videos. This created a distinct boundary between the left and right video sources that needed to be smoothened out in the section of our model dealing with image stitching.
			</p>
        </div>
        
        <! Panorma Subsection>
        <div class="subsection" id="image-stitch">
            <h3>3.2 Image Stitching</h3>
			<p>
				To stitch the split frames back together we need any algorithm that is able to extract a set of matching points found within both frames. There exist many algorithms that can accomplish such a task, but we decided on using the SIFT algorithm as it offers a quick and efficient framework that we are both familiar with implementing. However, not all point pairs returned by the SIFT algorithm are going to be optimal. At its base, image stitching is an overdetermined least squares problem which attempts to solve the following equation, where h represents our homography matrix. $$ A\vec{h} = \vec{0} $$ To further constrain the problem we restrict h such that it can only have unit norm. With this new constrained least squares problem we are able to redefine our equation in the following form with the eigenvector corresponding to the smallest eigenvalue being the vector which minimizes the least squares loss. $$ A^{T}A \vec{h} = \lambda \vec{h} $$
                
			</p>
			<div class="container2">
				<figure>
				  <img src="imgs/data_uncropped.gif">
				  <figcaption>Uncropped</figcaption>
				</figure>
				<figure>
				  <img src="imgs/data_cropped.gif">
				  <figcaption>Cropped</figcaption>
				</figure>
			</div>
			<div class="container2">
				<figure>
				  <img src="imgs/data_no_blend.gif">
				  <figcaption>Unblended</figcaption>
				</figure>
				<figure>
				  <img src="imgs/data_blend.gif">
				  <figcaption>Blended</figcaption>
				</figure>
			</div>
        </div>
        
        <! Object Tracking Subsection>
        <div class="subsection" id="object-tracking">
            <h3>3.3 Object Tracking</h3>
			<p>
				
			</p>
			<div class="img-center">
			<figure>
			<img src="imgs/kcf_tracker.png">
			<figcaption>KCF Tracker Architecture</figcaption>
			</figure>
			</div>
        </div>
        
        <! Results Section>
        <div id="results">
            <h2>4.0 Results</h2>
			<p>
				
			</p>
			<div class="container2">
				<figure>
				  <img src="imgs/track_orig.gif">
				  <figcaption>Original Tracking</figcaption>
				</figure>
				<figure>
				  <img src="imgs/track_stitched.gif">
				  <figcaption>Stitched Tracking</figcaption>
				</figure>
			</div>
			<div class="container2">
				<figure>
				  <img src="imgs/fail_bright.gif">
				  <figcaption>Failure 1</figcaption>
				</figure>
				<figure>
				  <img src="imgs/fail_overlap.gif">
				  <figcaption>Failure 2</figcaption>
				</figure>
			</div>
        </div>
        
        <! Conclusion Section>
        <div id="conclusion">
            <h2>5.0 Conclusion</h2>
        </div>
        
        <! Observations Subsection>
        <div class="subsection" id="observations">
            <h3>5.1 Observations</h3>
        </div>
        
        <! Challenges Subsection>
        <div class="subsection" id="challenges">
            <h3>5.2 Challenges</h3>
        </div>
        
        <! Future Work Subsection>
        <div class="subsection" id="future-work">
            <h3>5.3 Future Work</h3>
        </div>
        
        <! References Section>
        <div id="references">
            <h2>6.0 References</h2>
			<p>
			<ol>
				<li><p class="ref-indent">Yanfang Li, Yaming Wang, Wenqing Huang and Zuoli Zhang, <b>"Automatic image stitching using SIFT."</b> 2008 International Conference on Audio, Language and Image Processing, pp. 568-571. IEEE, 2008.</p></li>
				<li><p class="ref-indent">Bolme, David S., J. Ross Beveridge, Bruce A. Draper, and Yui Man Lui. <b>"Visual object tracking using adaptive correlation filters."</b> 2010 IEEE computer society conference on computer vision and pattern recognition, pp. 2544-2550. IEEE, 2010.</p></li>
				<li><p class="ref-indent">Yang, Yuebin, and Guillaume-Alexandre Bilodeau. <b>"Multiple object tracking with kernelized correlation filters in urban mixed traffic."</b> 2017 14th Conference on Computer and Robot Vision (CRV), pp. 209-216. IEEE, 2017.</p></li>
				<li><p class="ref-indent">Held, David & Thrun, Sebastian & Savarese, Silvio. (2016). <b>"Learning to Track at 100 FPS with Deep Regression Networks."</b> https://arxiv.org/abs/1604.01802</p></li> 
			</ol>
			</p>
        </div>
    </body>
</html>
